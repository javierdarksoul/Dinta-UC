{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0c7066",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Javier Rojas Herrera**\n",
    "\n",
    "jrojash1995@gmail.com\n",
    "\n",
    "**Hardware, Deployment y MLOps**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd3ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Letra pequeña del Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5a950",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Entrenamiento e Inferencia requieren de hardware avanzado en computo numérico**\n",
    "* Se requieren grandes volumenes de datos etiquetados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d9462",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "###  <center>[<img src=\"images/gpuvscpuvstpu.webp\" width=\"80%\"/> ](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482231c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU (graphics processing unit)\n",
    "###  <center>[<img src=\"images/A100.jpg\" width=\"60%\"/> ](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393142c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU vs CPU\n",
    "###  <center>[<img src=\"images/gpuvscpu.png\" width=\"60%\"/> ](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bc005",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU vs CPU: Inferencia\n",
    "###  <center>[<img src=\"images/gpuvscpu2.png\" width=\"70%\"/> ](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c279d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tabla resumen GPUS\n",
    "\n",
    "|GPU |  Cuda cores | Tensor cores | VRAM  | Power | Precio |\n",
    "|----------|----------|----------| ----------|  ----------| ----------|\n",
    "| T4    | 2500   | 320  | 15 GB | 70 W | 1100 usd |\n",
    "| L4   | 7680   | 240  | 24 GB | 72 W | 2600 usd |\n",
    "| L40   | 18176    | 568  | 48 GB | 300 W | 8400 usd |\n",
    "| A100    | 6920   |  422   | 80 GB | 400 W | 12000 usd |\n",
    "| H100    | 14592    |  456    | 80 GB | 350 W | 30000 usd |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511dbfa3",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 09:44:53 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3080        Off | 00000000:07:00.0 Off |                  N/A |\r\n",
      "|  0%   35C    P8              20W / 320W |    224MiB / 10240MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A       636      G   /usr/lib/Xorg                               116MiB |\r\n",
      "|    0   N/A  N/A       663      G   /usr/bin/sddm-greeter                        94MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4af301",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from torchvision.models import vit_b_16 , ViT_B_16_Weights\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from trans import UnNormalize\n",
    "from io import StringIO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb708db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "##load model VIT B 16\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "model = vit_b_16(weights=weights)\n",
    "model.heads.head = torch.nn.Linear(768,10)\n",
    "\n",
    "##set optimizer and loss\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "##load data into dataloader\n",
    "data = CIFAR10(\"./\", download=True, train=True, transform=weights.transforms())\n",
    "data = CIFAR10(\"./\", download=True, train=True, transform=weights.transforms())\n",
    "subset_indices = torch.randperm(len(data))[:1000]\n",
    "subset_cifar10 = Subset(data, subset_indices)\n",
    "dataloader = DataLoader(subset_cifar10, batch_size=32, shuffle=False)\n",
    "\n",
    "class_names = [\"Airplane\",\"Auto\",\"Bird\",\"Cat\",\"Deer\",\"Dog\",\"Frog\",\"Horse\",\"Ship\",\"Truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d3f6d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbb6d1e008047c5a75b21ac6ee9c442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, description='x', max=3000, min=-1000), Output()), _dom_classes=('w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "std = weights.transforms().std\n",
    "mean = weights.transforms().mean\n",
    "invTrans=UnNormalize(mean,std)\n",
    "@interact\n",
    "def show_articles_more_than(x=1000):\n",
    "    plt.figure(figsize=(5,3))\n",
    "    print(\"Label: \",class_names[data[x][1]])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(invTrans(data[x][0]).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41b7f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Qué elementos utilizan VRAM en un entrenamiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419c299",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "\n",
    "- Almacenamiento de tensores de entrada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f92446",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Almacenamiento de los parametros del modelo (weight and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b82f92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Almacenamiento de gradientes (backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4783062e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Almacenamiento de tensores de salida\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66490d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 09:44:58 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3080        Off | 00000000:07:00.0 Off |                  N/A |\r\n",
      "|  0%   36C    P2              27W / 320W |    463MiB / 10240MiB |      2%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A       636      G   /usr/lib/Xorg                               116MiB |\r\n",
      "|    0   N/A  N/A       663      G   /usr/bin/sddm-greeter                        94MiB |\r\n",
      "|    0   N/A  N/A     51977      C   ...vier/miniconda3/envs/UC2/bin/python      234MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "## Tomando un batch del dataloader y transfiriendolo a VRAM\n",
    "for image,label in dataloader:\n",
    "    image_=image.cuda()\n",
    "    break\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5afe6e21",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 09:44:59 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3080        Off | 00000000:07:00.0 Off |                  N/A |\r\n",
      "|  0%   38C    P2              59W / 320W |    845MiB / 10240MiB |      7%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A       636      G   /usr/lib/Xorg                               116MiB |\r\n",
      "|    0   N/A  N/A       663      G   /usr/bin/sddm-greeter                        94MiB |\r\n",
      "|    0   N/A  N/A     51977      C   ...vier/miniconda3/envs/UC2/bin/python      616MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "## transfiriendo el modelo a VRAM\n",
    "model = model.cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd70d0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Locura de los parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9f2dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# <center>[ <img src=\"images/madness.png\" width=\"60%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c311d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Cómo aprovechar al maximo el hardware disponible?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526bd38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ¿Cómo los computadores almacenan los numeros reales?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8259d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Estandar IEEE754 establece la forma en la que los numeros reales son almacenados en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305cf6ab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Existen los puntos flotante de 16,32,64,128 bits. Siendo el más utilizado el punto flotante de 32 bits o de precisión simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7f58a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# FP32 vs FP16\n",
    "\n",
    " # <center>[ <img src=\"images/fp16.ppm\" width=\"80%\"/>](attachment:image.png)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633c03e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FP32 expresión notación científica\n",
    "${(−1)^S × 2^{(E-127)} × 1.F}$   \n",
    "\n",
    "Donde:\n",
    "\n",
    "$S = signo$\n",
    "\n",
    "$E = exponente$\n",
    "\n",
    "$F = mantisa$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0f0c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FP16 expresión notación científica\n",
    "${(−1)^S × 2^{(E-15)} × 1.F}$   \n",
    "\n",
    "Donde:\n",
    "\n",
    "$S = signo$\n",
    "\n",
    "$E = exponente$\n",
    "\n",
    "$F = mantisa$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2ef933",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.234567891234567838e-01\n",
      "1.234567910432815552e-01\n",
      "1.234741210937500000e-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number= 0.123456789123456789123\n",
    "sio = StringIO()\n",
    "np.savetxt(sio, np.array([number], dtype=np.float64))\n",
    "np.savetxt(sio, np.array([number], dtype=np.float32))\n",
    "np.savetxt(sio, np.array([number], dtype=np.float16))\n",
    "s = sio.getvalue()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bcd847",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# FP32 vs FP16\n",
    "\n",
    " # <center>[ <img src=\"images/fp32vsfp16_tabla.png\" width=\"80%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad268d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "A= np.random.rand(5000,5000).astype(np.float32)\n",
    "B= np.random.rand(5000,5000).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c385d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1252.1611 1225.4093 1235.9021 ... 1236.6078 1240.352  1248.9011]\n",
      " [1253.1814 1260.9763 1243.3846 ... 1252.232  1248.1768 1273.7944]\n",
      " [1234.982  1225.7998 1223.8563 ... 1234.9598 1223.2595 1242.1187]\n",
      " ...\n",
      " [1264.1403 1249.7083 1258.812  ... 1251.8722 1247.9136 1274.4338]\n",
      " [1243.8434 1240.2603 1227.7488 ... 1228.9077 1222.6744 1249.3503]\n",
      " [1241.6654 1239.8179 1214.8789 ... 1242.3411 1235.4119 1242.6171]]\n",
      "CPU times: user 4.46 s, sys: 22.6 ms, total: 4.49 s\n",
      "Wall time: 375 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(np.matmul(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "147a2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "A= np.random.rand(5000,5000).astype(np.float64)\n",
    "B= np.random.rand(5000,5000).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78bd3290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1249.69263886 1264.36179625 1236.71745103 ... 1244.55155038\n",
      "  1239.56847115 1241.85963489]\n",
      " [1268.12400729 1265.97013354 1271.70727464 ... 1263.82112103\n",
      "  1248.87865961 1263.65998639]\n",
      " [1224.42082935 1234.9236422  1216.74991232 ... 1213.03525978\n",
      "  1216.03054201 1228.31899213]\n",
      " ...\n",
      " [1250.85675417 1253.5641412  1240.10554609 ... 1259.07204147\n",
      "  1240.8052213  1252.44091154]\n",
      " [1231.70855692 1241.47769454 1235.34692057 ... 1227.44051865\n",
      "  1225.86423203 1237.94136802]\n",
      " [1254.15307367 1265.07278831 1240.04607975 ... 1251.95226251\n",
      "  1251.75534882 1249.40062106]]\n",
      "CPU times: user 9.66 s, sys: 342 ms, total: 10 s\n",
      "Wall time: 837 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(np.matmul(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9056b51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Es posible usar representacion de FP16 para entrenar o inferir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b47e02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Algunas operaciones como las convoluciones o lineales, pueden realizarse completamente en FP16\n",
    "* Sin embargo, otras operaciones como la reducción, a menudo pueden necesitar la representacion en FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59780faf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision mixta automática (AMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3bcfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " # <center>[ <img src=\"images/amp.png\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731e260",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Realmente tiene beneficios usar FP16?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630988b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  # Tensor cores\n",
    " \n",
    " # <center>[ <img src=\"images/tensorop.png\" width=100%/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f7aa9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Tensor cores\n",
    " \n",
    " # <center>[ <img src=\"images/tensor_cores.gif\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348002cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entrenamiento tradicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11ec33a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo por epoca: 9.394094944000244 segs | Epoch loss: 80.61469578742981\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    time_i=time.time()\n",
    "    epoch_loss = 0.0\n",
    "    for image,label in dataloader:\n",
    "        optim.zero_grad() \n",
    "        image=image.cuda()  \n",
    "        label=label.cuda()\n",
    "        output = model(image)   \n",
    "        loss= cross_entropy(output,label)        \n",
    "        loss.backward()     \n",
    "        epoch_loss+=loss.item()\n",
    "        optim.step()\n",
    "    print(f'Tiempo por epoca: {time.time()-time_i} segs | Epoch loss: {epoch_loss}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a70444",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenamiento utilizando precision mixta + tensor cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96607197",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo por epoca: 4.504239559173584 segs | Epoch loss: 70.95635986328125\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    time_i=time.time()\n",
    "    epoch_loss = 0.0\n",
    "    for image,label in dataloader:\n",
    "        optim.zero_grad()\n",
    "        image=image.cuda()\n",
    "        label=label.cuda()\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            output = model(image)\n",
    "            loss= cross_entropy(output,label)        \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        epoch_loss+=loss.item()\n",
    "    print(f'Tiempo por epoca: {time.time()-time_i} segs | Epoch loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89933f04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué problemas pueden ocurrir al trabajar con una precisión de 16 bits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b883b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cálculo de gradientes acumulativos podrian no poder representarse en FP16 (Desvanecimiento de gradiente)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407d177",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entrenamiento con cálculo de gradiente escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5871447",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29.4912], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler =torch.amp.GradScaler(\"cuda\") \n",
    "a=torch.tensor([0.00045],requires_grad=True).cuda()\n",
    "scaler.scale(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "664e93a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo por epoca: 4.535714387893677 segs | Epoch loss: 62.873382568359375\n",
      "Tiempo por epoca: 4.51518988609314 segs | Epoch loss: 61.40594482421875\n",
      "Tiempo por epoca: 4.515941143035889 segs | Epoch loss: 59.82152557373047\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.amp.GradScaler(\"cuda\") \n",
    "for epoch in range(3):\n",
    "    time_i=time.time()\n",
    "    epoch_loss = 0.0\n",
    "    for image,label in dataloader:\n",
    "        optim.zero_grad()\n",
    "        image=image.cuda()\n",
    "        label=label.cuda()\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            output = model(image)\n",
    "            loss= cross_entropy(output,label)        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        epoch_loss+=loss.item()\n",
    "    print(f'Tiempo por epoca: {time.time()-time_i} segs | Epoch loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604ad6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Qué sucede si no dispongo de hardware o si requiero de pocas horas de computo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c4f655",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principales servicios cloud para creación de máquinas virtuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15ca853",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# <center>[<img src=\"images/azurevs.jpg\" width=\"80%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74abeb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pros de utilizar máquinas virtuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786eac3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Fácil de crear y configurar según las necesidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbb43e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Costo bajo al corto plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4dbaa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Integración directa con otros servicios cloud del mismo prestador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e265c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Contras de utilizar máquinas virtuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2eaba5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Los recursos solicitados pueden no estar disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188734d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Alto costo a largo plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d64bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Usar MV es lo más eficiente para realizar tareas de machine learning en la nube?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb725c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Los modelos en MV no escalan (Inferencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbabbc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Entrenamiento y despliegue de modelos complejo de automatizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193de02",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>[<img src=\"images/mlstudiovsvertex.png\" width=\"60%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f834b9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ventajas al utilizar servicios especializados para ML en la nube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ca49f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Deployment escalable y automatizado de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52cea1d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Entrenamiento automatizado (pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75710396",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Disponibilidad de una familia de modelos pre entrenados a través de API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb77f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Creación de notebooks jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e80fbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>[<img src=\"images/mlsteps.jpg\" width=\"80%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e688d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deployment de modelos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b71e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Disponibilizar modelos para el uso real de usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54ab0f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# <center>[<img src=\"images/depl.png\" width=\"50%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffa015",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modelo como API\n",
    "# <center>[<img src=\"images/apimodel.png\" width=\"70%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f12fcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frameworks para deployment de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136814f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>[<img src=\"images/deploy.png\" width=\"70%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb3f17",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo:  Bento ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cd376b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-20T09:54:12-0400 [WARNING] [cli] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:12-0400 [INFO] [cli] Environ for worker 0: set CUDA_VISIBLE_DEVICES to 0\n",
      "2024-08-20T09:54:12-0400 [INFO] [cli] Prometheus metrics for HTTP BentoServer from \"server:svc\" can be accessed at http://localhost:11000/metrics.\n",
      "2024-08-20T09:54:13-0400 [INFO] [cli] Starting production HTTP BentoServer from \"server:svc\" listening on http://0.0.0.0:11000 (Press CTRL+C to quit)\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:6] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:4] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:10] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:1] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:8] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:3] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:2] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [runner:gpt2:1] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:12] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:9] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:11] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:5] Using lowercased runnable class name 'gpt2' for runner.\n",
      "2024-08-20T09:54:16-0400 [WARNING] [api_server:7] Using lowercased runnable class name 'gpt2' for runner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/javier/miniconda3/envs/UC2/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system_raw('BENTOML_PORT=11000 bentoml serve server:svc &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709736f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modelo como API en VM\n",
    "# <center>[<img src=\"images/depl2.png\" width=\"70%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e228f26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problemas de levantar modelos API en MV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be601f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# <center>[<img src=\"images/apin2.png\" width=\"70%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49af86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solución: Escalar modelos API en cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde2323",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>[<img src=\"images/depl4.png\" width=\"50%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fab7b9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Mostrar ejemplo de escalamiento en VERTEX AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1656866",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué es MLOps?\n",
    "\n",
    "* Paradigma repetible que tiene como objetivo implementar y mantener modelos de aprendizaje automático en producción de manera confiable y eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fea088",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>[<img src=\"images/mlops.png\" width=\"80%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852230bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines en MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7daf03c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Una Pipeline es un flujo de trabajo conformado por uno o varios componentes y sus interacciones a través de entradas y salidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03c66a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>[<img src=\"images/compo.png\" width=\"60%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7da78b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>[<img src=\"images/pipeline.png\" width=\"60%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9a7e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frameworks para MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911964e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# <center>[<img src=\"images/mlops_frame2.png\" width=\"70%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df6e25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Servicios cloud para MLOps\n",
    "# <center>[<img src=\"images/mlstudiovsvertex.png\" width=\"60%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e24b6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo de pipeline de juguete definida en Kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e232a17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "@dsl.component\n",
    "def add(a: float, b: float) -> float:\n",
    "    return a + b\n",
    "\n",
    "@dsl.component\n",
    "def mul(a: float, b: float) -> float:\n",
    "    return a * b\n",
    "\n",
    "@dsl.pipeline\n",
    "def add_pipeline(a: float, b: float):\n",
    "    add_task = add(a=a, b=b)\n",
    "    mul_task = mul(a=a, b=add_task.output)\n",
    "    \n",
    "compiler.Compiler().compile(pipeline_func=add_pipeline, package_path='add_pipeline.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227b99a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo de Pipeline real en vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3295f0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# <center>[<img src=\"images/vertex.png\" width=\"56%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc1ec4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Niveles de MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f80a1f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nivel 0\n",
    "# <center>[<img src=\"images/nivel0.svg\" width=\"80%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ab672",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nivel 1\n",
    "# <center>[<img src=\"images/nivel1.svg\" width=\"70%\"/>](attachment:image.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3cf8cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nivel 2\n",
    "# <center>[<img src=\"images/nivel2.svg\" width=\"75%\"/>](attachment:image.png)</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
